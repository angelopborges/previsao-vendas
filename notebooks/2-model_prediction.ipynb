{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from xgboost import XGBRegressor\n",
    "import lightgbm as lgb\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, make_scorer\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "import joblib # Importar joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.read_csv('data\\df_tratado.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Início da Seleção e Avaliação Inicial de Modelos ===\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Início da Seleção e Avaliação Inicial de Modelos ===\")\n",
    "\n",
    "markdown_cols_existing = ['MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']\n",
    "existing_markdown_cols = [col for col in markdown_cols_existing if col in df_final.columns]\n",
    "\n",
    "\n",
    "#Preparando os dados para o modelo\n",
    "y = df_final['Weekly_Sales']\n",
    "sample_weights = df_final['Sample_Weight']\n",
    "\n",
    "features_to_use = [\n",
    "    'Store', 'Dept', 'Size', 'Temperature', 'Fuel_Price', 'CPI', 'Unemployment',\n",
    "    'IsHoliday_Flag',\n",
    "    'Year', 'Month', 'Week', 'Day', 'DayOfWeek', 'DayOfYear',\n",
    "    'SuperBowl', 'LaborDay', 'Thanksgiving', 'Christmas',\n",
    "    'TotalMarkDown', 'HasAnyMarkDown'\n",
    "]\n",
    "\n",
    "for col in existing_markdown_cols:\n",
    "    if f'Has_{col}' in df_final.columns and f'Has_{col}' not in features_to_use:\n",
    "        features_to_use.append(f'Has_{col}')\n",
    "    if col in df_final.columns and col not in features_to_use:\n",
    "        features_to_use.append(col)\n",
    "\n",
    "X = df_final[features_to_use]\n",
    "\n",
    "categorical_features = ['Store', 'Dept']\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dados divididos temporalmente:\n",
      "Tamanho do conjunto de treino: 208238 amostras\n",
      "Tamanho do conjunto de teste: 52060 amostras\n",
      "Período de treino: 2011-02-04 a 2012-06-29\n",
      "Período de teste: 2012-06-29 a 2012-10-26\n",
      "\n",
      "Shape de X_train após pré-processamento: (208238, 148)\n",
      "Shape de X_test após pré-processamento: (52060, 148)\n"
     ]
    }
   ],
   "source": [
    "#Dividindo os dados para Treino e Teste\n",
    "df_final_sorted = df_final.sort_values(by='Date').reset_index(drop=True)\n",
    "X_sorted = df_final_sorted[features_to_use]\n",
    "y_sorted = df_final_sorted['Weekly_Sales']\n",
    "sample_weights_sorted = df_final_sorted['Sample_Weight']\n",
    "\n",
    "train_size = int(len(df_final_sorted) * 0.8)\n",
    "X_train_raw, X_test_raw = X_sorted.iloc[:train_size], X_sorted.iloc[train_size:]\n",
    "y_train, y_test = y_sorted.iloc[:train_size], y_sorted.iloc[train_size:]\n",
    "sample_weights_train, sample_weights_test = sample_weights_sorted.iloc[:train_size], sample_weights_sorted.iloc[train_size:]\n",
    "\n",
    "print(f\"\\nDados divididos temporalmente:\")\n",
    "print(f\"Tamanho do conjunto de treino: {X_train_raw.shape[0]} amostras\")\n",
    "print(f\"Tamanho do conjunto de teste: {X_test_raw.shape[0]} amostras\")\n",
    "print(f\"Período de treino: {df_final_sorted['Date'].iloc[0]} a {df_final_sorted['Date'].iloc[train_size-1]}\")\n",
    "print(f\"Período de teste: {df_final_sorted['Date'].iloc[train_size]} a {df_final_sorted['Date'].iloc[-1]}\")\n",
    "\n",
    "X_train = preprocessor.fit_transform(X_train_raw)\n",
    "X_test = preprocessor.transform(X_test_raw)\n",
    "\n",
    "if isinstance(X_train, np.ndarray):\n",
    "    pass\n",
    "else:\n",
    "    X_train = X_train.toarray()\n",
    "    X_test = X_test.toarray()\n",
    "\n",
    "print(f\"\\nShape de X_train após pré-processamento: {X_train.shape}\")\n",
    "print(f\"Shape de X_test após pré-processamento: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Função para calcular e avaliar as métricas\n",
    "def evaluate_model(y_true, y_pred, model_name=\"Modelo\"):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "    print(f\"\\n=== Métricas de Avaliação para {model_name} ===\")\n",
    "    print(f\"MAE: {mae:.2f}\")\n",
    "    print(f\"RMSE: {rmse:.2f}\")\n",
    "    print(f\"MAPE: {mape:.2f}%\")\n",
    "    return mae, rmse, mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iniciando treinamento e avaliação do modelo de Regressão Linear (Baseline)...\n",
      "\n",
      "=== Métricas de Avaliação para Regressão Linear ===\n",
      "MAE: 8045.77\n",
      "RMSE: 12257.81\n",
      "MAPE: 8565.40%\n"
     ]
    }
   ],
   "source": [
    "# === Adicionando a Regressão Linear Simples como Baseline ===\n",
    "print(\"\\nIniciando treinamento e avaliação do modelo de Regressão Linear (Baseline)...\")\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(X_train, y_train, sample_weight=sample_weights_train)\n",
    "y_pred_linear = linear_model.predict(X_test)\n",
    "mae_linear, rmse_linear, mape_linear = evaluate_model(y_test, y_pred_linear, \"Regressão Linear\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iniciando treinamento e avaliação do XGBoost Regressor...\n",
      "\n",
      "=== Métricas de Avaliação para XGBoost Regressor ===\n",
      "MAE: 3414.25\n",
      "RMSE: 5525.24\n",
      "MAPE: 4011.90%\n"
     ]
    }
   ],
   "source": [
    "#Treinamento do XGBoost Regressor\n",
    "print(\"\\nIniciando treinamento e avaliação do XGBoost Regressor...\")\n",
    "xgb_model = XGBRegressor(random_state=42, n_jobs=-1)\n",
    "xgb_model.fit(X_train, y_train, sample_weight=sample_weights_train)\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "mae_xgb, rmse_xgb, mape_xgb = evaluate_model(y_test, y_pred_xgb, \"XGBoost Regressor\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iniciando treinamento e avaliação do LightGBM Regressor...\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008845 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2948\n",
      "[LightGBM] [Info] Number of data points in the train set: 208238, number of used features: 147\n",
      "[LightGBM] [Info] Start training from score 16854.559579\n",
      "\n",
      "=== Métricas de Avaliação para LightGBM Regressor ===\n",
      "MAE: 4383.45\n",
      "RMSE: 6726.22\n",
      "MAPE: 5076.13%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projetos\\ChallengeDS\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Treinamento do LightGBM Regressor\n",
    "print(\"\\nIniciando treinamento e avaliação do LightGBM Regressor...\")\n",
    "lgbm_model = lgb.LGBMRegressor(random_state=42, n_jobs=-1)\n",
    "lgbm_model.fit(X_train, y_train, sample_weight=sample_weights_train)\n",
    "y_pred_lgbm = lgbm_model.predict(X_test)\n",
    "mae_lgbm, rmse_lgbm, mape_lgbm = evaluate_model(y_test, y_pred_lgbm, \"LightGBM Regressor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Comparação Final de Modelos ===\n",
      "Regressão Linear MAPE: 8565.40%\n",
      "XGBoost MAPE: 4011.90%\n",
      "LightGBM MAPE: 5076.13%\n",
      "\n",
      "O modelo 'XGBoost' apresentou o melhor MAPE inicial (4011.90%).\n",
      "\n",
      "=== Seleção e Avaliação Inicial de Modelos Concluída ===\n"
     ]
    }
   ],
   "source": [
    "#Comparação dos modelos\n",
    "print(\"\\n=== Comparação Final de Modelos ===\")\n",
    "print(f\"Regressão Linear MAPE: {mape_linear:.2f}%\")\n",
    "print(f\"XGBoost MAPE: {mape_xgb:.2f}%\")\n",
    "print(f\"LightGBM MAPE: {mape_lgbm:.2f}%\")\n",
    "\n",
    "#Melhor modelo com base no MAPE\n",
    "best_mape = min(mape_linear, mape_xgb, mape_lgbm)\n",
    "if best_mape == mape_linear:\n",
    "    best_model_name = \"Regressão Linear\"\n",
    "elif best_mape == mape_xgb:\n",
    "    best_model_name = \"XGBoost\"\n",
    "else:\n",
    "    best_model_name = \"LightGBM\"\n",
    "\n",
    "print(f\"\\nO modelo '{best_model_name}' apresentou o melhor MAPE inicial ({best_mape:.2f}%).\")\n",
    "\n",
    "print(\"\\n=== Seleção e Avaliação Inicial de Modelos Concluída ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Início da Otimização de Hiperparâmetros do Modelo (XGBoost) ===\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Início da Otimização de Hiperparâmetros do Modelo (XGBoost) ===\")\n",
    "\n",
    "#Criando um scorer customizado para MAPE\n",
    "def mape_scorer(y_true, y_pred):\n",
    "    return -np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "mape_neg_scorer = make_scorer(mape_scorer, greater_is_better=True)\n",
    "\n",
    "\n",
    "#busca de hiperparâmetros para XGBoost\n",
    "param_dist = {\n",
    "    'n_estimators': [100, 200, 300, 400], # Número de árvores\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2], # Taxa de aprendizado\n",
    "    'max_depth': [3, 5, 7, 9], # Profundidade máxima da árvore\n",
    "    'subsample': [0.6, 0.8, 1.0], # Fração de amostras por árvore\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0], # Fração de features por árvore\n",
    "    'gamma': [0, 0.1, 0.2], # Mínima redução de perda para uma divisão\n",
    "    'reg_alpha': [0, 0.1, 0.5], # L1 regularization\n",
    "    'reg_lambda': [0.5, 1, 1.5] # L2 regularization\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iniciando busca RandomizedSearchCV com 50 iterações e 3-fold CV...\n"
     ]
    }
   ],
   "source": [
    "#Inicializando o modelo base\n",
    "xgb_base_model = XGBRegressor(random_state=42, n_jobs=-1)\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=xgb_base_model,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=50, # Reduzido para 50 iterações para um teste inicial mais rápido. 200 travou localmente.\n",
    "    cv=3,  \n",
    "    scoring=mape_neg_scorer,\n",
    "    verbose=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1   #Usando todos os núcleos da CPU para paralelizar\n",
    ")\n",
    "\n",
    "print(f\"\\nIniciando busca RandomizedSearchCV com {random_search.n_iter} iterações e {random_search.cv}-fold CV...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projetos\\ChallengeDS\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:881: UserWarning: The scoring make_scorer(mape_scorer, response_method='predict') does not support sample_weight, which may lead to statistically incorrect results when fitting RandomizedSearchCV(cv=3,\n",
      "                   estimator=XGBRegressor(base_score=None, booster=None,\n",
      "                                          callbacks=None,\n",
      "                                          colsample_bylevel=None,\n",
      "                                          colsample_bynode=None,\n",
      "                                          colsample_bytree=None, device=None,\n",
      "                                          early_stopping_rounds=None,\n",
      "                                          enable_categorical=False,\n",
      "                                          eval_metric=None, feature_types=None,\n",
      "                                          feature_weights=None, gamma=None,\n",
      "                                          grow_policy=None,\n",
      "                                          importance_type=None,\n",
      "                                          interaction_constraint...\n",
      "                                          num_parallel_tree=None, ...),\n",
      "                   n_iter=50, n_jobs=-1,\n",
      "                   param_distributions={'colsample_bytree': [0.6, 0.8, 1.0],\n",
      "                                        'gamma': [0, 0.1, 0.2],\n",
      "                                        'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
      "                                        'max_depth': [3, 5, 7, 9],\n",
      "                                        'n_estimators': [100, 200, 300, 400],\n",
      "                                        'reg_alpha': [0, 0.1, 0.5],\n",
      "                                        'reg_lambda': [0.5, 1, 1.5],\n",
      "                                        'subsample': [0.6, 0.8, 1.0]},\n",
      "                   random_state=42,\n",
      "                   scoring=make_scorer(mape_scorer, response_method='predict'),\n",
      "                   verbose=2) with sample_weight. \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "\n",
      "=== Otimização Concluída ===\n",
      "\n",
      "Melhores Hiperparâmetros Encontrados para XGBoost:\n",
      "{'subsample': 0.6, 'reg_lambda': 1.5, 'reg_alpha': 0, 'n_estimators': 400, 'max_depth': 9, 'learning_rate': 0.2, 'gamma': 0.1, 'colsample_bytree': 1.0}\n"
     ]
    }
   ],
   "source": [
    "#Otimização\n",
    "random_search.fit(X_train, y_train, sample_weight=sample_weights_train)\n",
    "\n",
    "print(\"\\n=== Otimização Concluída ===\")\n",
    "\n",
    "# Obter os melhores parâmetros e o melhor modelo\n",
    "best_xgb_params = random_search.best_params_\n",
    "best_xgb_model = random_search.best_estimator_\n",
    "\n",
    "print(\"\\nMelhores Hiperparâmetros Encontrados para XGBoost:\")\n",
    "print(best_xgb_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Avaliação do Melhor Modelo XGBoost Encontrado (no conjunto de teste):\n",
      "\n",
      "=== Métricas de Avaliação para XGBoost Otimizado ===\n",
      "MAE: 1998.32\n",
      "RMSE: 3483.95\n",
      "MAPE: 2276.73%\n",
      "\n",
      "MAPE Inicial do XGBoost (não otimizado): 15057.52%\n",
      "MAPE Otimizado do XGBoost: 2276.73%\n",
      "\n",
      "=== Otimização de Hiperparâmetros do Modelo Concluída ===\n"
     ]
    }
   ],
   "source": [
    "#Avaliando o Melhor Modelo no conjunto de teste\n",
    "print(\"\\nAvaliação do Melhor Modelo XGBoost Encontrado (no conjunto de teste):\")\n",
    "y_pred_best_xgb = best_xgb_model.predict(X_test)\n",
    "mae_best_xgb, rmse_best_xgb, mape_best_xgb = evaluate_model(y_test, y_pred_best_xgb, \"XGBoost Otimizado\")\n",
    "\n",
    "print(f\"\\nMAPE Inicial do XGBoost (não otimizado): {15057.52:.2f}%\") # Relembrando o MAPE do card anterior\n",
    "print(f\"MAPE Otimizado do XGBoost: {mape_best_xgb:.2f}%\")\n",
    "\n",
    "print(\"\\n=== Otimização de Hiperparâmetros do Modelo Concluída ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Início da Geração das Previsões para o Próximo Ano ===\n",
      "Total de combinações únicas de Loja/Departamento: 2983\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Início da Geração das Previsões para o Próximo Ano ===\")\n",
    "\n",
    "#Combinações únicas de Loja/Departamento\n",
    "unique_store_dept = df_final[['Store', 'Dept']].drop_duplicates().sort_values(by=['Store', 'Dept']).reset_index(drop=True)\n",
    "print(f\"Total de combinações únicas de Loja/Departamento: {len(unique_store_dept)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datas de previsão: de 2012-11-02 a 2013-11-01\n"
     ]
    }
   ],
   "source": [
    "# O último ano nos dados é 2012 (dado que o dataset original vai até 26/10/2012).\n",
    "# Vamos prever para o período imediatamente após a última data até completar 52 semanas.\n",
    "df_final['Date'] = pd.to_datetime(df_final['Date'])\n",
    "\n",
    "last_date_in_data = df_final['Date'].max()\n",
    "# ==============================\n",
    "\n",
    "prediction_start_date = last_date_in_data + timedelta(weeks=1) \n",
    "prediction_end_date = prediction_start_date + timedelta(weeks=52)\n",
    "\n",
    "# O dataset original parece ter datas de sexta-feira.\n",
    "future_dates = pd.date_range(start=prediction_start_date, end=prediction_end_date, freq='W-FRI')\n",
    "\n",
    "print(f\"Datas de previsão: de {future_dates.min().strftime('%Y-%m-%d')} a {future_dates.max().strftime('%Y-%m-%d')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame futuro de base criado. Shape: (158099, 3)\n",
      "        Date  Store  Dept\n",
      "0 2012-11-02      1     1\n",
      "1 2012-11-09      1     1\n",
      "2 2012-11-16      1     1\n",
      "3 2012-11-23      1     1\n",
      "4 2012-11-30      1     1\n"
     ]
    }
   ],
   "source": [
    "#Juntando num dataframe com todas as combinações de loja/depto e datas futuras\n",
    "future_df = pd.DataFrame()\n",
    "for store_dept_idx, row in unique_store_dept.iterrows():\n",
    "    temp_df = pd.DataFrame({'Date': future_dates,\n",
    "                            'Store': row['Store'],\n",
    "                            'Dept': row['Dept']})\n",
    "    future_df = pd.concat([future_df, temp_df], ignore_index=True)\n",
    "\n",
    "print(f\"\\nDataFrame futuro de base criado. Shape: {future_df.shape}\")\n",
    "print(future_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Features Temporais\n",
    "future_df['Year'] = future_df['Date'].dt.year\n",
    "future_df['Month'] = future_df['Date'].dt.month\n",
    "future_df['Week'] = future_df['Date'].dt.isocalendar().week.astype(int)\n",
    "future_df['Day'] = future_df['Date'].dt.day\n",
    "future_df['DayOfWeek'] = future_df['Date'].dt.dayofweek\n",
    "future_df['DayOfYear'] = future_df['Date'].dt.dayofyear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Features de Feriado (IsHoliday_Flag, SuperBowl, LaborDay, Thanksgiving, Christmas)\n",
    "\n",
    "future_df['IsHoliday'] = False # Default para não-feriado\n",
    "# As datas exatas de feriados variam por ano, mas as semanas são consistentes.\n",
    "# Vamos assumir que a coluna IsHoliday do arquivo features já reflete os feriados nos anos passados.\n",
    "# Para o futuro, podemos usar uma heurística baseada nas datas históricas de IsHoliday=True\n",
    "# Uma forma robusta seria ter um calendário de feriados exato para 2013, mas para este exercício, replicaremos a lógica.\n",
    "\n",
    "holiday_weeks_months = df_final[df_final['IsHoliday'] == True][['Month', 'Week']].drop_duplicates()\n",
    "\n",
    "future_df['IsHoliday'] = future_df.apply(\n",
    "    lambda row: True if any((row['Month'] == hw['Month']) and (row['Week'] == hw['Week']) for idx, hw in holiday_weeks_months.iterrows()) else False,\n",
    "    axis=1\n",
    ")\n",
    "future_df['IsHoliday_Flag'] = future_df['IsHoliday'].astype(int)\n",
    "\n",
    "#Flags de feriados específicos (SuperBowl, LaborDay, Thanksgiving, Christmas)\n",
    "future_df['SuperBowl'] = ((future_df['Month'] == 2) & (future_df['Week'].isin([6, 7])) & (future_df['IsHoliday'] == True)).astype(int)\n",
    "future_df['LaborDay'] = ((future_df['Month'] == 9) & (future_df['Week'].isin([36])) & (future_df['IsHoliday'] == True)).astype(int)\n",
    "future_df['Thanksgiving'] = ((future_df['Month'] == 11) & (future_df['Week'].isin([47])) & (future_df['IsHoliday'] == True)).astype(int)\n",
    "future_df['Christmas'] = ((future_df['Month'] == 12) & (future_df['Week'].isin([51, 52])) & (future_df['IsHoliday'] == True)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Features econômicas/climáticas preenchidas com médias históricas por semana.\n"
     ]
    }
   ],
   "source": [
    "#Features Econômicas/Climáticas (Temperature, Fuel_Price, CPI, Unemployment)\n",
    "#Usando a média histórica por semana do ano\n",
    "#Calculando as médias do dataset de treino para evitar vazamento\n",
    "historical_weekly_avg = df_final.groupby('Week')[['Temperature', 'Fuel_Price', 'CPI', 'Unemployment']].mean().reset_index()\n",
    "\n",
    "future_df = pd.merge(future_df, historical_weekly_avg, on='Week', how='left')\n",
    "print(\"\\nFeatures econômicas/climáticas preenchidas com médias históricas por semana.\")\n",
    "\n",
    "#Features MarkDown (TotalMarkDown, HasAnyMarkDown, Has_MarkDownX)\n",
    "#Assumindo 0 para todas as promoções no futuro para uma previsão baseline\n",
    "for col in markdown_cols_existing:\n",
    "    future_df[col] = 0.0\n",
    "    future_df[f'Has_{col}'] = 0 \n",
    "df_final['TotalMarkDown'] = df_final[existing_markdown_cols].sum(axis=1)\n",
    "future_df['TotalMarkDown'] = 0.0\n",
    "future_df['HasAnyMarkDown'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Features futuras geradas e preparadas. Exemplo:\n",
      "   Store  Dept    Size  Temperature  Fuel_Price         CPI  Unemployment  \\\n",
      "0      1     1  151315    48.532614    3.522661  172.664150      7.767738   \n",
      "1      1     1  151315    48.737653    3.491016  172.741792      7.771354   \n",
      "2      1     1  151315    51.750548    3.489257  172.856435      7.766710   \n",
      "3      1     1  151315    49.113817    3.435670  172.965630      7.767530   \n",
      "4      1     1  151315    45.616886    3.395459  173.092511      7.771769   \n",
      "\n",
      "   IsHoliday_Flag  Year  Month  ...  Has_MarkDown1  MarkDown1  Has_MarkDown2  \\\n",
      "0               0  2012     11  ...              0        0.0              0   \n",
      "1               0  2012     11  ...              0        0.0              0   \n",
      "2               0  2012     11  ...              0        0.0              0   \n",
      "3               1  2012     11  ...              0        0.0              0   \n",
      "4               0  2012     11  ...              0        0.0              0   \n",
      "\n",
      "   MarkDown2  Has_MarkDown3  MarkDown3  Has_MarkDown4  MarkDown4  \\\n",
      "0        0.0              0        0.0              0        0.0   \n",
      "1        0.0              0        0.0              0        0.0   \n",
      "2        0.0              0        0.0              0        0.0   \n",
      "3        0.0              0        0.0              0        0.0   \n",
      "4        0.0              0        0.0              0        0.0   \n",
      "\n",
      "   Has_MarkDown5  MarkDown5  \n",
      "0              0        0.0  \n",
      "1              0        0.0  \n",
      "2              0        0.0  \n",
      "3              0        0.0  \n",
      "4              0        0.0  \n",
      "\n",
      "[5 rows x 30 columns]\n",
      "Shape de future_X_raw: (158099, 30)\n",
      "Colunas de future_X_raw: ['Store', 'Dept', 'Size', 'Temperature', 'Fuel_Price', 'CPI', 'Unemployment', 'IsHoliday_Flag', 'Year', 'Month', 'Week', 'Day', 'DayOfWeek', 'DayOfYear', 'SuperBowl', 'LaborDay', 'Thanksgiving', 'Christmas', 'TotalMarkDown', 'HasAnyMarkDown', 'Has_MarkDown1', 'MarkDown1', 'Has_MarkDown2', 'MarkDown2', 'Has_MarkDown3', 'MarkDown3', 'Has_MarkDown4', 'MarkDown4', 'Has_MarkDown5', 'MarkDown5']\n"
     ]
    }
   ],
   "source": [
    "#Features da Loja ('Size', 'Type')\n",
    "df_stores = pd.read_csv('D:/Projetos/ChallengeDS/data/raw/stores data-set.csv')\n",
    "future_df = pd.merge(future_df, df_stores[['Store', 'Type', 'Size']], on='Store', how='left')\n",
    "\n",
    "if 'Type' in future_df.columns:\n",
    "    future_df['Type'] = future_df['Type'].astype('category')\n",
    "\n",
    "\n",
    "# Organizando as colunas do future_df para corresponder à ordem das features no treinamento\n",
    "# X_sorted.columns contém as features na ordem esperada pelo preprocessor\n",
    "# X_train_raw é um dataframe, então tem nomes de colunas que preprocessor aprendeu.\n",
    "# Devemos garantir que future_df_processed tenha as mesmas colunas na mesma ordem\n",
    "future_X_raw = future_df[features_to_use]\n",
    "\n",
    "print(\"\\nFeatures futuras geradas e preparadas. Exemplo:\")\n",
    "print(future_X_raw.head())\n",
    "print(f\"Shape de future_X_raw: {future_X_raw.shape}\")\n",
    "print(f\"Colunas de future_X_raw: {future_X_raw.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shape de X_future_processed após pré-processamento: (158099, 148)\n"
     ]
    }
   ],
   "source": [
    "#Pré-processamento\n",
    "#Usando o preprocessor JÁ FITADO nos dados de treino.\n",
    "X_future_processed = preprocessor.transform(future_X_raw)\n",
    "\n",
    "if hasattr(X_future_processed, 'toarray'):\n",
    "    X_future_processed = X_future_processed.toarray()\n",
    "\n",
    "print(f\"\\nShape de X_future_processed após pré-processamento: {X_future_processed.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gerando previsões de vendas com o modelo XGBoost otimizado...\n",
      "\n",
      "Previsões geradas e valores negativos ajustados para 0.\n"
     ]
    }
   ],
   "source": [
    "#Previsões\n",
    "print(\"\\nGerando previsões de vendas com o modelo XGBoost otimizado...\")\n",
    "future_predictions = best_xgb_model.predict(X_future_processed)\n",
    "\n",
    "future_predictions[future_predictions < 0] = 0\n",
    "print(\"\\nPrevisões geradas e valores negativos ajustados para 0.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Exemplo de Previsões Geradas ===\n",
      "   Store  Dept       Date  Predicted_Weekly_Sales\n",
      "0      1     1 2012-11-02            38250.226562\n",
      "1      1     1 2012-11-09            34385.386719\n",
      "2      1     1 2012-11-16            33070.886719\n",
      "3      1     1 2012-11-23            35606.609375\n",
      "4      1     1 2012-11-30            34301.164062\n",
      "5      1     1 2012-12-07            33047.320312\n",
      "6      1     1 2012-12-14            42374.613281\n",
      "7      1     1 2012-12-21            40778.207031\n",
      "8      1     1 2012-12-28            34227.746094\n",
      "9      1     1 2013-01-04            19969.082031\n",
      "\n",
      "Total de previsões geradas: 158099\n",
      "Período das previsões: 2012-11-02 00:00:00 a 2013-11-01 00:00:00\n",
      "\n",
      "=== Geração das Previsões para o Próximo Ano Concluída ===\n"
     ]
    }
   ],
   "source": [
    "#Formatando a saída\n",
    "df_future_predictions = future_df[['Store', 'Dept', 'Date']].copy()\n",
    "df_future_predictions['Predicted_Weekly_Sales'] = future_predictions\n",
    "\n",
    "print(\"\\n=== Exemplo de Previsões Geradas ===\")\n",
    "print(df_future_predictions.head(10)) #as primeiras 10 previsões\n",
    "\n",
    "print(f\"\\nTotal de previsões geradas: {len(df_future_predictions)}\")\n",
    "print(f\"Período das previsões: {df_future_predictions['Date'].min()} a {df_future_predictions['Date'].max()}\")\n",
    "\n",
    "print(\"\\n=== Geração das Previsões para o Próximo Ano Concluída ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Início da Serialização e Salvamento do Modelo e Pré-processador ===\n",
      "Salvando o modelo XGBoost otimizado em: models\\best_xgb_model.joblib\n",
      "Modelo salvo com sucesso.\n",
      "Salvando o pré-processador em: models\\preprocessor.joblib\n",
      "Pré-processador salvo com sucesso.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Início da Serialização e Salvamento do Modelo e Pré-processador ===\")\n",
    "\n",
    "models_dir = 'models'\n",
    "os.makedirs(models_dir, exist_ok=True) \n",
    "\n",
    "model_path = os.path.join(models_dir, 'best_xgb_model.joblib')\n",
    "preprocessor_path = os.path.join(models_dir, 'preprocessor.joblib')\n",
    "\n",
    "print(f\"Salvando o modelo XGBoost otimizado em: {model_path}\")\n",
    "joblib.dump(best_xgb_model, model_path)\n",
    "print(\"Modelo salvo com sucesso.\")\n",
    "\n",
    "print(f\"Salvando o pré-processador em: {preprocessor_path}\")\n",
    "joblib.dump(preprocessor, preprocessor_path)\n",
    "print(\"Pré-processador salvo com sucesso.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Verificando se os arquivos podem ser carregados de volta...\n",
      "Modelo e pré-processador carregados com sucesso para verificação.\n",
      "Tipo do modelo carregado: <class 'xgboost.sklearn.XGBRegressor'>\n",
      "Tipo do pré-processador carregado: <class 'sklearn.compose._column_transformer.ColumnTransformer'>\n",
      "\n",
      "=== Serialização e Salvamento do Modelo e Pré-processador Concluída ===\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nVerificando se os arquivos podem ser carregados de volta...\")\n",
    "try:\n",
    "    loaded_model = joblib.load(model_path)\n",
    "    loaded_preprocessor = joblib.load(preprocessor_path)\n",
    "    print(\"Modelo e pré-processador carregados com sucesso para verificação.\")\n",
    "\n",
    "    print(f\"Tipo do modelo carregado: {type(loaded_model)}\")\n",
    "    print(f\"Tipo do pré-processador carregado: {type(loaded_preprocessor)}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao carregar os arquivos: {e}\")\n",
    "    print(\"Isso pode indicar um problema com o salvamento.\")\n",
    "\n",
    "print(\"\\n=== Serialização e Salvamento do Modelo e Pré-processador Concluída ===\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
